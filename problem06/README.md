# Problem 6

## Part A
### Instructions:
Open the file `Face_Emotion_Recognition_from_Videos.ipynb` in  your preferred notebook application.

## Part B:
### Instructions:
```py
python b.py
```

### Sample Stream:
<iframe width="560" height="315" src="https://www.youtube.com/embed/CKdoJ6MOMag" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Part D:
All outputs for part D are in the folder `my_output`, which contains the processed live feeds of 2 people.
### Instructions:
This is the same project as part two. It is modified to accept multiple subjects.
```
python b.py
```

### Sample Stream:
<iframe width="560" height="315" src="https://www.youtube.com/embed/ijTBZYLy1Jc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Development Process:

At first, I ran the sample code, then I placed my files (some records from myself) inside the code, both images and videos. After which I uploaded the output here in the output folder but the tables and plot placed in the report file. For other conceptual questions, I put the relevant answers in the report file.
For a short faces-recording along with the script that can read live webcam streams,b.py file has been written.
